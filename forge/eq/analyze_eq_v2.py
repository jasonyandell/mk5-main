#!/usr/bin/env python3
"""Analyze E[Q] predictions from v2.2+ datasets with self-contained actual_outcome.

This script works with datasets generated by generate_dataset.py v2.2+ which
include player and actual_outcome fields directly - no game replay needed.

Usage:
    python -m forge.eq.analyze_eq_v2 forge/data/eq_v2.2_250g.pt
"""

from __future__ import annotations

import argparse
from pathlib import Path

import numpy as np
import torch
from scipy import stats


def analyze_dataset(dataset_path: Path) -> dict:
    """Load dataset and analyze E[Q] predictions.

    Returns dict with analysis results.
    """
    print(f"Loading dataset: {dataset_path}")
    data = torch.load(dataset_path, weights_only=False)

    metadata = data['metadata']
    print(f"  Version: {metadata['version']}")
    print(f"  {metadata['n_games']} games, {metadata['n_examples']} decisions")

    # Extract tensors
    e_q_mean = data['e_q_mean'].numpy()  # (N, 7)
    e_q_var = data['e_q_var'].numpy()    # (N, 7)
    action_taken = data['action_taken'].numpy()  # (N,)
    player = data['player'].numpy()       # (N,)
    actual_outcome = data['actual_outcome'].numpy()  # (N,)
    legal_mask = data['legal_mask'].numpy()  # (N, 7)
    decision_idx = data['decision_idx'].numpy()  # (N,)

    n_decisions = len(action_taken)

    # Get E[Q] and variance for actions that were taken
    predictions = np.array([e_q_mean[i, action_taken[i]] for i in range(n_decisions)])
    variances = np.array([e_q_var[i, action_taken[i]] for i in range(n_decisions)])
    stds = np.sqrt(variances)

    # Compute errors (prediction - actual)
    errors = predictions - actual_outcome

    # Hand size from decision_idx (decision 0-3 = hand 7, 4-7 = hand 6, etc.)
    hand_sizes = 7 - (decision_idx // 4)

    # Overall metrics
    mae = np.abs(errors).mean()
    rmse = np.sqrt((errors ** 2).mean())
    mean_error = errors.mean()
    std_error = errors.std()

    # Correlation
    correlation, p_value = stats.pearsonr(predictions, actual_outcome)

    # Uncertainty calibration
    within_1sigma = np.abs(errors) <= stds
    within_2sigma = np.abs(errors) <= 2 * stds
    coverage_1sigma = within_1sigma.mean()
    coverage_2sigma = within_2sigma.mean()

    return {
        'n_games': metadata['n_games'],
        'n_decisions': n_decisions,
        'mae': mae,
        'rmse': rmse,
        'mean_error': mean_error,
        'std_error': std_error,
        'correlation': correlation,
        'correlation_pvalue': p_value,
        'coverage_1sigma': coverage_1sigma,
        'coverage_2sigma': coverage_2sigma,
        'mean_uncertainty': stds.mean(),
        'predictions': predictions,
        'actual_outcome': actual_outcome,
        'errors': errors,
        'hand_sizes': hand_sizes,
        'player': player,
        'stds': stds,
        'metadata': metadata,
    }


def print_results(results: dict) -> None:
    """Print analysis results in a readable format."""
    print("\n" + "=" * 60)
    print("E[Q] PREDICTION ANALYSIS (v2.2 schema)")
    print("=" * 60)

    print(f"\nDataset: {results['n_games']} games, {results['n_decisions']} decisions")

    print("\n--- Overall Accuracy ---")
    print(f"  MAE:              {results['mae']:.2f} points")
    print(f"  RMSE:             {results['rmse']:.2f} points")
    print(f"  Mean Error:       {results['mean_error']:+.2f} points (calibration)")
    print(f"  Std Error:        {results['std_error']:.2f} points")

    print("\n--- Correlation ---")
    print(f"  Pearson r:        {results['correlation']:.3f}")
    print(f"  p-value:          {results['correlation_pvalue']:.2e}")

    print("\n--- Uncertainty Calibration ---")
    print(f"  Mean σ:           {results['mean_uncertainty']:.2f} points")
    print(f"  Coverage ±1σ:     {results['coverage_1sigma']:.1%} (expect ~68%)")
    print(f"  Coverage ±2σ:     {results['coverage_2sigma']:.1%} (expect ~95%)")

    # By hand size
    print("\n--- Accuracy by Hand Size (Game Phase) ---")
    hand_sizes = results['hand_sizes']
    errors = results['errors']
    predictions = results['predictions']
    actual = results['actual_outcome']

    for hs in sorted(set(hand_sizes), reverse=True):
        mask = hand_sizes == hs
        hs_errors = errors[mask]
        hs_mae = np.abs(hs_errors).mean()
        hs_bias = hs_errors.mean()
        hs_pred = predictions[mask].mean()
        hs_actual = actual[mask].mean()
        n = mask.sum()
        phase = "early" if hs >= 5 else ("mid" if hs >= 3 else "late")
        print(f"  Hand {hs} ({phase:5s}): MAE={hs_mae:5.2f}, bias={hs_bias:+6.2f}, "
              f"pred={hs_pred:+5.1f}, actual={hs_actual:+5.1f} (n={n})")

    # By player
    print("\n--- Accuracy by Player ---")
    player = results['player']
    for p in range(4):
        mask = player == p
        if mask.sum() > 0:
            p_errors = errors[mask]
            print(f"  Player {p}: MAE={np.abs(p_errors).mean():.2f}, "
                  f"mean_err={p_errors.mean():+.2f} (n={mask.sum()})")

    # Bias diagnosis
    print("\n--- Bias Diagnosis ---")
    print(f"  Mean prediction:  {predictions.mean():+.1f} points")
    print(f"  Mean actual:      {actual.mean():+.1f} points")
    print(f"  Calibration:      {results['mean_error']:+.2f} points")

    # Bias by prediction quartile
    pred_quartiles = np.percentile(predictions, [25, 50, 75])
    print(f"\n  Bias by prediction quartile:")
    ranges = [
        ("Low (< Q1)", predictions < pred_quartiles[0]),
        ("Med-Low", (predictions >= pred_quartiles[0]) & (predictions < pred_quartiles[1])),
        ("Med-High", (predictions >= pred_quartiles[1]) & (predictions < pred_quartiles[2])),
        ("High (> Q3)", predictions >= pred_quartiles[2]),
    ]
    for name, mask in ranges:
        if mask.sum() > 0:
            print(f"    {name:15s}: bias={errors[mask].mean():+.1f}, n={mask.sum()}")

    # Exploration stats from metadata
    if 'exploration' in results['metadata']:
        exp = results['metadata']['exploration']
        if exp.get('enabled'):
            print("\n--- Exploration Stats (from metadata) ---")
            summary = results['metadata'].get('summary', {}).get('exploration_stats', {})
            print(f"  Greedy rate:      {summary.get('greedy_rate', 0):.1%}")
            print(f"  Mean Q-gap:       {summary.get('mean_q_gap', 0):.2f} points")
            modes = summary.get('mode_counts', {})
            print(f"  Mode counts:      greedy={modes.get('greedy', 0)}, "
                  f"boltzmann={modes.get('boltzmann', 0)}, "
                  f"epsilon={modes.get('epsilon', 0)}, "
                  f"blunder={modes.get('blunder', 0)}")


def main():
    parser = argparse.ArgumentParser(description="Analyze E[Q] dataset (v2.2+ schema)")
    parser.add_argument("dataset", type=str, help="Path to dataset .pt file")
    parser.add_argument("--verbose", "-v", action="store_true", help="Show sample decisions")

    args = parser.parse_args()

    results = analyze_dataset(Path(args.dataset))
    print_results(results)

    if args.verbose:
        print("\n--- Sample Decisions ---")
        for i in range(min(10, results['n_decisions'])):
            print(f"  {i}: pred={results['predictions'][i]:+.1f}, "
                  f"actual={results['actual_outcome'][i]:+.1f}, "
                  f"error={results['errors'][i]:+.1f}, "
                  f"player={results['player'][i]}")


if __name__ == "__main__":
    main()
