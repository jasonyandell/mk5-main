{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02a: Entropy Decomposition\n",
    "\n",
    "**Goal**: Understand which features reduce uncertainty about V.\n",
    "\n",
    "**Key Questions**:\n",
    "1. Which features eliminate the most uncertainty?\n",
    "2. Does any feature set drive H(V|features) -> 0? (V is deterministic function)\n",
    "3. Diminishing returns curve: how many features to capture 90%, 99%, 99.9%?\n",
    "\n",
    "**Reference**: docs/analysis-draft.md Section 3.1"
   ]
  },
  {
   "cell_type": "code",
   "source": "# === CONFIGURATION ===\nDATA_DIR = \"/mnt/d/shards-standard/\"\nPROJECT_ROOT = \"/home/jason/v2/mk5-tailwind\"\n\n# === Setup imports ===\nimport sys\nif PROJECT_ROOT not in sys.path:\n    sys.path.insert(0, PROJECT_ROOT)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\nfrom forge.analysis.utils import loading, features, compression, viz\nfrom forge.oracle import schema\n\nviz.setup_notebook_style()\nprint(\"✓ Ready\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load multiple seeds for robust analysis\nshard_files = loading.find_shard_files(DATA_DIR)\nN_SEEDS = min(5, len(shard_files))  # Use fewer seeds due to memory\nsample_files = shard_files[:N_SEEDS]\nprint(f\"Analyzing {N_SEEDS} shards\")"
  },
  {
   "cell_type": "code",
   "source": "# Load and combine shards with sampling to manage memory\n# Each shard has millions of states; sample to keep total manageable\n\nSAMPLE_PER_SHARD = 50_000  # 50k samples per shard = 250k total for 5 seeds\nprint(f\"Loading {N_SEEDS} shards ({SAMPLE_PER_SHARD:,} samples each)...\")\n\ndfs = []\nfor path in tqdm(sample_files, desc=\"Loading shards\"):\n    df, seed, decl_id = schema.load_file(path)\n    \n    # Sample if shard is large\n    if len(df) > SAMPLE_PER_SHARD:\n        df = df.sample(n=SAMPLE_PER_SHARD, random_state=seed)\n    \n    # Add metadata\n    df['seed'] = seed\n    df['decl_id'] = decl_id\n    dfs.append(df)\n\ncombined_df = pd.concat(dfs, ignore_index=True)\nprint(f\"✓ Built combined_df: {len(combined_df):,} sampled states from {N_SEEDS} shards\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all features\n",
    "states = combined_df['state'].values\n",
    "V = combined_df['V'].values\n",
    "\n",
    "# Basic features (fast)\n",
    "depth_vals = features.depth(states)\n",
    "team_vals = features.team(states).astype(int)\n",
    "player_vals = features.player(states)\n",
    "balance_vals = features.hand_balance(states)\n",
    "\n",
    "# Trick info\n",
    "leader_vals, trick_len_vals = features.trick_info(states)\n",
    "\n",
    "print(f\"Features extracted for {len(states):,} states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Count domino features (per-seed, slower)\n# For multi-seed analysis, we need to track seed\ncounts_remaining_vals = np.zeros(len(states), dtype=np.int32)\nteam0_counts_vals = np.zeros(len(states), dtype=np.int32)\nteam1_counts_vals = np.zeros(len(states), dtype=np.int32)\n\n# Process per-seed (convert numpy int64 to Python int for Random)\nfor seed in tqdm(combined_df['seed'].unique(), desc=\"Computing count features\"):\n    seed_int = int(seed)  # Convert numpy int64 to Python int\n    mask = combined_df['seed'].values == seed\n    seed_states = states[mask]\n    counts_remaining_vals[mask] = features.counts_remaining(seed_states, seed_int)\n    t0, t1 = features.counts_by_team(seed_states, seed_int)\n    team0_counts_vals[mask] = t0\n    team1_counts_vals[mask] = t1\n\nprint(\"Count features computed\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total entropy of V\n",
    "H_V = compression.entropy_bits(V)\n",
    "max_H = np.log2(85)  # 85 possible values\n",
    "\n",
    "print(f\"H(V) = {H_V:.4f} bits\")\n",
    "print(f\"Max possible H = {max_H:.4f} bits\")\n",
    "print(f\"Efficiency: {100*H_V/max_H:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conditional Entropy Analysis\n",
    "\n",
    "Compute H(V|feature) for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build feature dict\n",
    "feature_dict = {\n",
    "    'depth': depth_vals,\n",
    "    'team': team_vals,\n",
    "    'player': player_vals,\n",
    "    'leader': leader_vals,\n",
    "    'trick_len': trick_len_vals,\n",
    "    'hand_balance': balance_vals,\n",
    "    'counts_remaining': counts_remaining_vals,\n",
    "    'team0_counts': team0_counts_vals,\n",
    "    'team1_counts': team1_counts_vals,\n",
    "    'seed': combined_df['seed'].values,\n",
    "    'decl_id': combined_df['decl_id'].values,\n",
    "}\n",
    "\n",
    "# Compute information gain for each\n",
    "info_results = compression.information_gain_ranking(V, feature_dict)\n",
    "\n",
    "print(\"\\nInformation gain ranking:\")\n",
    "print(f\"{'Feature':<20} {'I(V;F)':<10} {'H(V|F)':<10} {'Reduction':<10}\")\n",
    "print(\"-\" * 50)\n",
    "for name, mi, h_cond in info_results:\n",
    "    reduction = 100 * mi / H_V\n",
    "    print(f\"{name:<20} {mi:<10.4f} {h_cond:<10.4f} {reduction:<10.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot information gain\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "viz.plot_entropy_curve(info_results, ax=ax, title=\"Information Gain by Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../results/figures/02a_info_gain.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cumulative Information Gain\n",
    "\n",
    "How much entropy reduction from combining features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy feature selection - add features one by one\n",
    "# This is expensive, so we'll use a subset\n",
    "\n",
    "def greedy_feature_selection(V, feature_dict, max_features=5):\n",
    "    \"\"\"Greedily select features to minimize conditional entropy.\"\"\"\n",
    "    remaining = set(feature_dict.keys())\n",
    "    selected = []\n",
    "    history = [('(none)', H_V, 0)]\n",
    "    \n",
    "    current_key = None  # Combined feature key\n",
    "    \n",
    "    for i in range(max_features):\n",
    "        best_name = None\n",
    "        best_h_cond = H_V\n",
    "        \n",
    "        for name in remaining:\n",
    "            # Combine with already selected features\n",
    "            if current_key is None:\n",
    "                combined = feature_dict[name].astype(str)\n",
    "            else:\n",
    "                combined = np.char.add(current_key.astype(str), '_' + feature_dict[name].astype(str))\n",
    "            \n",
    "            h_cond = compression.conditional_entropy(V, combined)\n",
    "            if h_cond < best_h_cond:\n",
    "                best_h_cond = h_cond\n",
    "                best_name = name\n",
    "                best_combined = combined\n",
    "        \n",
    "        if best_name is None:\n",
    "            break\n",
    "            \n",
    "        selected.append(best_name)\n",
    "        remaining.remove(best_name)\n",
    "        current_key = best_combined\n",
    "        reduction = 100 * (H_V - best_h_cond) / H_V\n",
    "        history.append(('+' + best_name, best_h_cond, reduction))\n",
    "        \n",
    "        print(f\"Selected: {best_name}, H(V|features) = {best_h_cond:.4f}, reduction = {reduction:.1f}%\")\n",
    "    \n",
    "    return selected, history\n",
    "\n",
    "# Run greedy selection\n",
    "print(f\"\\nGreedy feature selection (starting H(V) = {H_V:.4f}):\\n\")\n",
    "selected_features, selection_history = greedy_feature_selection(V, feature_dict, max_features=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative reduction\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Conditional entropy curve\n",
    "steps = [h[0] for h in selection_history]\n",
    "h_conds = [h[1] for h in selection_history]\n",
    "reductions = [h[2] for h in selection_history]\n",
    "\n",
    "axes[0].plot(range(len(h_conds)), h_conds, 'o-', markersize=8)\n",
    "axes[0].set_xticks(range(len(steps)))\n",
    "axes[0].set_xticklabels(steps, rotation=45, ha='right')\n",
    "axes[0].set_ylabel('H(V|features) (bits)')\n",
    "axes[0].set_title('Conditional Entropy vs Features Added')\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Cumulative reduction\n",
    "axes[1].plot(range(len(reductions)), reductions, 'o-', markersize=8, color='green')\n",
    "axes[1].set_xticks(range(len(steps)))\n",
    "axes[1].set_xticklabels(steps, rotation=45, ha='right')\n",
    "axes[1].set_ylabel('% Entropy Reduction')\n",
    "axes[1].set_title('Cumulative Information Gain')\n",
    "axes[1].axhline(y=90, color='orange', linestyle='--', alpha=0.5, label='90%')\n",
    "axes[1].axhline(y=99, color='red', linestyle='--', alpha=0.5, label='99%')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../results/figures/02a_cumulative_info.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Interaction Analysis\n",
    "\n",
    "Are there synergies between features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise information gain matrix\n",
    "top_features = ['depth', 'seed', 'team', 'counts_remaining', 'leader']\n",
    "\n",
    "# Single feature gains\n",
    "single_gains = {}\n",
    "for f in top_features:\n",
    "    single_gains[f] = H_V - compression.conditional_entropy(V, feature_dict[f])\n",
    "\n",
    "# Pairwise gains\n",
    "pair_gains = np.zeros((len(top_features), len(top_features)))\n",
    "for i, f1 in enumerate(top_features):\n",
    "    for j, f2 in enumerate(top_features):\n",
    "        if i <= j:\n",
    "            combined = np.char.add(feature_dict[f1].astype(str), '_' + feature_dict[f2].astype(str))\n",
    "            pair_gains[i, j] = H_V - compression.conditional_entropy(V, combined)\n",
    "            pair_gains[j, i] = pair_gains[i, j]\n",
    "\n",
    "# Compute synergy: pair_gain - max(single_gain_1, single_gain_2)\n",
    "synergy = np.zeros_like(pair_gains)\n",
    "for i, f1 in enumerate(top_features):\n",
    "    for j, f2 in enumerate(top_features):\n",
    "        expected = max(single_gains[f1], single_gains[f2])\n",
    "        synergy[i, j] = pair_gains[i, j] - expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pairwise information gain heatmap\n",
    "sns.heatmap(pair_gains, annot=True, fmt='.3f', xticklabels=top_features, \n",
    "            yticklabels=top_features, ax=axes[0], cmap='YlOrRd')\n",
    "axes[0].set_title('Pairwise Information Gain I(V; F1,F2)')\n",
    "\n",
    "# Synergy heatmap\n",
    "sns.heatmap(synergy, annot=True, fmt='.3f', xticklabels=top_features,\n",
    "            yticklabels=top_features, ax=axes[1], cmap='RdBu_r', center=0)\n",
    "axes[1].set_title('Feature Synergy (excess over max single)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sufficient Statistics Analysis\n",
    "\n",
    "Can we find a small set that captures most information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we have (seed, depth, team)? This should capture most info\n",
    "# since seed determines the deal and depth+team determine game state\n",
    "\n",
    "key_features = ['seed', 'depth', 'team']\n",
    "combined_key = feature_dict['seed'].astype(str)\n",
    "for f in key_features[1:]:\n",
    "    combined_key = np.char.add(combined_key, '_' + feature_dict[f].astype(str))\n",
    "\n",
    "h_cond_key = compression.conditional_entropy(V, combined_key)\n",
    "reduction_key = 100 * (H_V - h_cond_key) / H_V\n",
    "\n",
    "print(f\"H(V | {', '.join(key_features)}) = {h_cond_key:.4f} bits\")\n",
    "print(f\"Reduction: {reduction_key:.1f}%\")\n",
    "print(f\"Remaining uncertainty: {h_cond_key:.4f} bits ({100*h_cond_key/H_V:.1f}% of original)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more features to key\n",
    "extended_keys = ['seed', 'depth', 'team', 'leader', 'trick_len']\n",
    "combined_ext = feature_dict['seed'].astype(str)\n",
    "for f in extended_keys[1:]:\n",
    "    combined_ext = np.char.add(combined_ext, '_' + feature_dict[f].astype(str))\n",
    "\n",
    "h_cond_ext = compression.conditional_entropy(V, combined_ext)\n",
    "reduction_ext = 100 * (H_V - h_cond_ext) / H_V\n",
    "\n",
    "print(f\"H(V | {', '.join(extended_keys)}) = {h_cond_ext:.4f} bits\")\n",
    "print(f\"Reduction: {reduction_ext:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Per-Seed Analysis\n",
    "\n",
    "Does the pattern hold across seeds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze each seed separately\n",
    "seed_entropy_results = []\n",
    "\n",
    "for seed in combined_df['seed'].unique():\n",
    "    mask = combined_df['seed'].values == seed\n",
    "    V_seed = V[mask]\n",
    "    depth_seed = depth_vals[mask]\n",
    "    \n",
    "    h_v = compression.entropy_bits(V_seed)\n",
    "    h_v_depth = compression.conditional_entropy(V_seed, depth_seed)\n",
    "    \n",
    "    seed_entropy_results.append({\n",
    "        'seed': seed,\n",
    "        'H_V': h_v,\n",
    "        'H_V_depth': h_v_depth,\n",
    "        'reduction_depth': 100 * (h_v - h_v_depth) / h_v,\n",
    "    })\n",
    "\n",
    "seed_entropy_df = pd.DataFrame(seed_entropy_results)\n",
    "print(\"Per-seed entropy analysis:\")\n",
    "print(seed_entropy_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find how many features needed for 90%, 99%\n",
    "features_for_90 = None\n",
    "features_for_99 = None\n",
    "for i, (name, h, red) in enumerate(selection_history[1:], 1):\n",
    "    if red >= 90 and features_for_90 is None:\n",
    "        features_for_90 = i\n",
    "    if red >= 99 and features_for_99 is None:\n",
    "        features_for_99 = i\n",
    "\n",
    "summary = {\n",
    "    'Total states': f\"{len(V):,}\",\n",
    "    'H(V) baseline': f\"{H_V:.4f} bits\",\n",
    "    'Top feature': info_results[0][0],\n",
    "    'Top feature I(V;F)': f\"{info_results[0][1]:.4f} bits\",\n",
    "    'Top feature reduction': f\"{100*info_results[0][1]/H_V:.1f}%\",\n",
    "    'Features for 90%': features_for_90 or 'N/A',\n",
    "    'Features for 99%': features_for_99 or 'N/A',\n",
    "    'Final H(V|features)': f\"{selection_history[-1][1]:.4f} bits\",\n",
    "    'Final reduction': f\"{selection_history[-1][2]:.1f}%\",\n",
    "}\n",
    "\n",
    "print(viz.create_summary_table(summary, \"Entropy Decomposition Summary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df = pd.DataFrame(info_results, columns=['feature', 'mutual_info', 'conditional_entropy'])\n",
    "results_df['reduction_pct'] = 100 * results_df['mutual_info'] / H_V\n",
    "results_df.to_csv('../../results/tables/02a_info_gain.csv', index=False)\n",
    "print(\"Results saved to results/tables/02a_info_gain.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}