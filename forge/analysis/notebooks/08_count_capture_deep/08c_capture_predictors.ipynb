{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 08c: Count Capture Predictors\n",
    "\n",
    "**Goal**: Identify what features predict count capture outcomes.\n",
    "\n",
    "**Key Question**: What predicts count capture?\n",
    "\n",
    "**Candidates**: Trump control, suit length, who holds the count, position.\n",
    "\n",
    "**Method**: Logistic regression or decision tree on count capture outcomes.\n",
    "\n",
    "**Output**: Feature importance for each count domino.\n",
    "\n",
    "**Key insight we're testing**: If counts are predictable from initial features, then the \"game\" is essentially decided at declaration time, not through clever play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "DATA_DIR = \"/mnt/d/shards-standard/\"\n",
    "PROJECT_ROOT = \"/home/jason/v2/mk5-tailwind\"\n",
    "\n",
    "# === Setup imports ===\n",
    "import sys\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from forge.analysis.utils import loading, features, viz, navigation\n",
    "from forge.oracle import schema, tables\n",
    "\n",
    "viz.setup_notebook_style()\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Feature Engineering\n",
    "\n",
    "For each seed/declaration, extract features that might predict count capture:\n",
    "\n",
    "1. **Who holds the count**: Is it on Team 0 or Team 1?\n",
    "2. **Trump control**: How many trumps does each team have?\n",
    "3. **Suit distribution**: Who has length in each suit?\n",
    "4. **Count domino specifics**: High vs low pip count dominoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_deal_features(seed, decl_id):\n",
    "    \"\"\"Extract features for a deal that might predict count capture.\"\"\"\n",
    "    hands = schema.deal_from_seed(seed)\n",
    "    \n",
    "    # Determine trump suit\n",
    "    # decl_id: 0-6 = suits (blank through sixes), 7 = follow-me, 8 = nello, 9 = splash\n",
    "    trump_suit = decl_id if decl_id <= 6 else -1\n",
    "    \n",
    "    features_dict = {\n",
    "        'seed': seed,\n",
    "        'decl_id': decl_id,\n",
    "    }\n",
    "    \n",
    "    # Count trumps per player\n",
    "    for p in range(4):\n",
    "        trump_count = 0\n",
    "        if trump_suit >= 0:\n",
    "            for domino_id in hands[p]:\n",
    "                pips = schema.domino_pips(domino_id)\n",
    "                if trump_suit in pips:\n",
    "                    trump_count += 1\n",
    "        features_dict[f'p{p}_trumps'] = trump_count\n",
    "    \n",
    "    # Team trump counts\n",
    "    features_dict['team0_trumps'] = features_dict['p0_trumps'] + features_dict['p2_trumps']\n",
    "    features_dict['team1_trumps'] = features_dict['p1_trumps'] + features_dict['p3_trumps']\n",
    "    features_dict['trump_advantage'] = features_dict['team0_trumps'] - features_dict['team1_trumps']\n",
    "    \n",
    "    # For each count domino, record who holds it\n",
    "    for domino_id in features.COUNT_DOMINO_IDS:\n",
    "        pips = schema.domino_pips(domino_id)\n",
    "        col_name = f'holder_{pips[0]}_{pips[1]}'\n",
    "        \n",
    "        holder = -1\n",
    "        for p in range(4):\n",
    "            if domino_id in hands[p]:\n",
    "                holder = p\n",
    "                break\n",
    "        \n",
    "        features_dict[col_name] = holder\n",
    "        features_dict[f'team_{pips[0]}_{pips[1]}'] = holder % 2 if holder >= 0 else -1\n",
    "        \n",
    "        # Is the count a trump?\n",
    "        is_trump = trump_suit in pips if trump_suit >= 0 else False\n",
    "        features_dict[f'is_trump_{pips[0]}_{pips[1]}'] = int(is_trump)\n",
    "    \n",
    "    return features_dict\n",
    "\n",
    "# Test\n",
    "test_features = extract_deal_features(0, 5)\n",
    "print(\"Sample features for seed=0, decl=5 (fives):\")\n",
    "for k, v in test_features.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Collect Training Data\n",
    "\n",
    "For each seed, get initial features and final count capture outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# Load shards and collect (features, capture_outcome) pairs\nshard_files = loading.find_shard_files(DATA_DIR)\n# Need more shards here since we only extract 1 data point per shard\n# Memory cleanup after each means we can handle more\nN_SHARDS = 20\n\nprint(f\"Processing {N_SHARDS} shards...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "# Collect data\ntraining_data = []\n\nfor shard_file in tqdm(shard_files[:N_SHARDS], desc=\"Processing shards\"):\n    df, seed, decl_id = schema.load_file(shard_file)\n    \n    # Build state lookup\n    state_to_idx, V, Q = navigation.build_state_lookup_fast(df)\n    states = df['state'].values\n    \n    # Get initial state (depth=28, all dominoes remaining)\n    depths = features.depth(states)\n    initial_mask = depths == 28\n    if not initial_mask.any():\n        del df, state_to_idx, V, Q, states\n        continue\n    \n    # Take first initial state\n    initial_idx = np.where(initial_mask)[0][0]\n    initial_state = states[initial_idx]\n    \n    # Track captures from initial state\n    captures = navigation.track_count_captures(\n        initial_state, seed, decl_id, state_to_idx, V, Q\n    )\n    \n    # Extract features\n    deal_features = extract_deal_features(seed, decl_id)\n    \n    # Add capture outcomes\n    for domino_id in features.COUNT_DOMINO_IDS:\n        pips = schema.domino_pips(domino_id)\n        col_name = f'capture_{pips[0]}_{pips[1]}'\n        \n        if domino_id in captures:\n            # 1 if team 0 captures, 0 if team 1\n            deal_features[col_name] = 1 if captures[domino_id] == 0 else 0\n        else:\n            deal_features[col_name] = np.nan  # Count not tracked (shouldn't happen)\n    \n    training_data.append(deal_features)\n    \n    # Clear memory\n    del df, state_to_idx, V, Q, states\n\ntrain_df = pd.DataFrame(training_data)\nprint(f\"Collected {len(train_df)} seed observations\")\nprint(f\"Columns: {list(train_df.columns)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Baseline: Who Holds It Predicts Who Gets It?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple analysis: Does holding the count predict capturing it?\n",
    "print(\"=== Baseline: Holder Predicts Capture ===\")\n",
    "\n",
    "for domino_id in features.COUNT_DOMINO_IDS:\n",
    "    pips = schema.domino_pips(domino_id)\n",
    "    points = tables.DOMINO_COUNT_POINTS[domino_id]\n",
    "    \n",
    "    team_col = f'team_{pips[0]}_{pips[1]}'\n",
    "    capture_col = f'capture_{pips[0]}_{pips[1]}'\n",
    "    \n",
    "    # Drop any NaN rows\n",
    "    valid = train_df[[team_col, capture_col]].dropna()\n",
    "    \n",
    "    # How often does holder's team capture?\n",
    "    holder_wins = (valid[team_col] == 0) & (valid[capture_col] == 1)\n",
    "    holder_wins |= (valid[team_col] == 1) & (valid[capture_col] == 0)\n",
    "    \n",
    "    accuracy = holder_wins.mean()\n",
    "    print(f\"{pips[0]}-{pips[1]} ({points}pts): Holder's team captures {accuracy*100:.1f}% of the time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. Logistic Regression for Each Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns for prediction\n",
    "feature_cols = [\n",
    "    'trump_advantage',\n",
    "    'team0_trumps',\n",
    "    'team1_trumps',\n",
    "]\n",
    "\n",
    "# Add per-count holder features\n",
    "for domino_id in features.COUNT_DOMINO_IDS:\n",
    "    pips = schema.domino_pips(domino_id)\n",
    "    feature_cols.append(f'team_{pips[0]}_{pips[1]}')\n",
    "    feature_cols.append(f'is_trump_{pips[0]}_{pips[1]}')\n",
    "\n",
    "print(f\"Feature columns: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit logistic regression for each count domino\n",
    "results = []\n",
    "\n",
    "for domino_id in features.COUNT_DOMINO_IDS:\n",
    "    pips = schema.domino_pips(domino_id)\n",
    "    points = tables.DOMINO_COUNT_POINTS[domino_id]\n",
    "    capture_col = f'capture_{pips[0]}_{pips[1]}'\n",
    "    \n",
    "    # Prepare data\n",
    "    valid = train_df[feature_cols + [capture_col]].dropna()\n",
    "    X = valid[feature_cols].values\n",
    "    y = valid[capture_col].values\n",
    "    \n",
    "    if len(np.unique(y)) < 2:\n",
    "        print(f\"{pips[0]}-{pips[1]}: Skipping (only one class)\")\n",
    "        continue\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Fit logistic regression\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_scaled, y, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # Fit on all data for coefficients\n",
    "    model.fit(X_scaled, y)\n",
    "    \n",
    "    results.append({\n",
    "        'domino': f\"{pips[0]}-{pips[1]}\",\n",
    "        'points': points,\n",
    "        'cv_accuracy': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'n_samples': len(y),\n",
    "        'model': model,\n",
    "        'feature_names': feature_cols,\n",
    "    })\n",
    "    \n",
    "    print(f\"{pips[0]}-{pips[1]} ({points}pts): CV accuracy = {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coefficients for feature importance\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    ax = axes[i]\n",
    "    model = result['model']\n",
    "    \n",
    "    # Get coefficients\n",
    "    coefs = model.coef_[0]\n",
    "    \n",
    "    # Sort by absolute value\n",
    "    sorted_idx = np.argsort(np.abs(coefs))[::-1]\n",
    "    \n",
    "    # Plot top features\n",
    "    top_n = min(10, len(coefs))\n",
    "    top_idx = sorted_idx[:top_n]\n",
    "    \n",
    "    colors = ['green' if c > 0 else 'red' for c in coefs[top_idx]]\n",
    "    ax.barh(range(top_n), coefs[top_idx], color=colors, alpha=0.7)\n",
    "    ax.set_yticks(range(top_n))\n",
    "    ax.set_yticklabels([feature_cols[j] for j in top_idx], fontsize=8)\n",
    "    ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax.set_xlabel('Coefficient')\n",
    "    ax.set_title(f\"{result['domino']} ({result['points']}pts)\\nCV Acc: {result['cv_accuracy']:.3f}\")\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "# Hide unused subplot\n",
    "if len(results) < 6:\n",
    "    axes[5].set_visible(False)\n",
    "\n",
    "plt.suptitle('Feature Importance for Count Capture Prediction', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../results/figures/08c_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. Random Forest for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try Random Forest for potentially better feature importance\n",
    "rf_results = []\n",
    "\n",
    "for domino_id in features.COUNT_DOMINO_IDS:\n",
    "    pips = schema.domino_pips(domino_id)\n",
    "    points = tables.DOMINO_COUNT_POINTS[domino_id]\n",
    "    capture_col = f'capture_{pips[0]}_{pips[1]}'\n",
    "    \n",
    "    # Prepare data\n",
    "    valid = train_df[feature_cols + [capture_col]].dropna()\n",
    "    X = valid[feature_cols].values\n",
    "    y = valid[capture_col].values\n",
    "    \n",
    "    if len(np.unique(y)) < 2:\n",
    "        continue\n",
    "    \n",
    "    # Fit Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "    cv_scores = cross_val_score(rf, X, y, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # Fit on all data for feature importance\n",
    "    rf.fit(X, y)\n",
    "    \n",
    "    rf_results.append({\n",
    "        'domino': f\"{pips[0]}-{pips[1]}\",\n",
    "        'points': points,\n",
    "        'cv_accuracy': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'feature_importance': rf.feature_importances_,\n",
    "    })\n",
    "    \n",
    "    print(f\"{pips[0]}-{pips[1]} ({points}pts): RF CV accuracy = {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "comparison_df = pd.DataFrame({\n",
    "    'domino': [r['domino'] for r in results],\n",
    "    'points': [r['points'] for r in results],\n",
    "    'logistic_acc': [r['cv_accuracy'] for r in results],\n",
    "    'rf_acc': [r['cv_accuracy'] for r in rf_results],\n",
    "})\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print()\n",
    "print(f\"Mean Logistic Regression accuracy: {comparison_df['logistic_acc'].mean():.3f}\")\n",
    "print(f\"Mean Random Forest accuracy: {comparison_df['rf_acc'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 7. Most Important Features Across All Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate feature importance across all counts\n",
    "all_importances = np.zeros(len(feature_cols))\n",
    "\n",
    "for result in rf_results:\n",
    "    all_importances += result['feature_importance']\n",
    "\n",
    "all_importances /= len(rf_results)\n",
    "\n",
    "# Sort by importance\n",
    "sorted_idx = np.argsort(all_importances)[::-1]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.barh(range(len(feature_cols)), all_importances[sorted_idx], color='steelblue', alpha=0.7)\n",
    "ax.set_yticks(range(len(feature_cols)))\n",
    "ax.set_yticklabels([feature_cols[i] for i in sorted_idx])\n",
    "ax.set_xlabel('Mean Feature Importance (Random Forest)')\n",
    "ax.set_title('Feature Importance for Count Capture Prediction (Averaged Across All Counts)')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../results/figures/08c_aggregate_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'Seeds analyzed': len(train_df),\n",
    "    'Mean Logistic CV accuracy': f\"{comparison_df['logistic_acc'].mean():.3f}\",\n",
    "    'Mean RF CV accuracy': f\"{comparison_df['rf_acc'].mean():.3f}\",\n",
    "    'Most important feature': feature_cols[sorted_idx[0]],\n",
    "    'Second most important': feature_cols[sorted_idx[1]],\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"08c SUMMARY: Count Capture Predictors\")\n",
    "print(\"=\"*60)\n",
    "for k, v in summary.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"KEY FINDING: Who holds the count is the strongest predictor.\")\n",
    "print(\"Trump advantage provides additional predictive power.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "comparison_df.to_csv('../../results/tables/08c_model_comparison.csv', index=False)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'mean_importance': all_importances,\n",
    "}).sort_values('mean_importance', ascending=False)\n",
    "importance_df.to_csv('../../results/tables/08c_feature_importance.csv', index=False)\n",
    "\n",
    "print(\"Results saved to:\")\n",
    "print(\"  - figures/08c_feature_importance.png\")\n",
    "print(\"  - figures/08c_aggregate_importance.png\")\n",
    "print(\"  - tables/08c_model_comparison.csv\")\n",
    "print(\"  - tables/08c_feature_importance.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}